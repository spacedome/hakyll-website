<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"
    xmlns:dc="http://purl.org/dc/elements/1.1/">
    <channel>
        <title>Spacedome.tv</title>
        <link>https://spacedome.tv/posts</link>
        <description><![CDATA[Recent Posts]]></description>
        <atom:link href="https://spacedome.tv/posts/rss.xml" rel="self"
                   type="application/rss+xml" />
        <lastBuildDate>Sat, 11 Jan 2025 00:00:00 UT</lastBuildDate>
        <item>
    <title>Trace Estimation</title>
    <link>https://spacedome.tv/posts/trace-estimation.html</link>
    <description><![CDATA[<main>
  <article>
    <header>
    <h1>Trace Estimation</h1>
    <p><i>2025-01-11 </i></p>
    <hr/>
    </header>
    <section>
      <p>The other day on twitter, I got involved in a discussion about trace estimation, and decided to write down some thoughts on the matter.</p>
<p>Typically this is posed as follows.
You are given a matrix <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo>∈</mo><msup><mi>F</mi><mrow><mi>N</mi><mo>×</mo><mi>N</mi></mrow></msup></mrow><annotation encoding="application/x-tex">A \in F^{N\times N}</annotation></semantics></math> such that we can only access it through matrix-vector multiplication <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>↦</mo><mi>A</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">x \mapsto Ax</annotation></semantics></math>. Estimate the trace of the matrix.
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">tr</mtext><mi>A</mi><mo>=</mo><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><msub><mi>A</mi><mrow><mi>i</mi><mo>,</mo><mi>i</mi></mrow></msub></mrow><annotation encoding="application/x-tex"> \text{tr} A = \sum_{i=1}^N A_{i,i} </annotation></semantics></math>
We may not be able to take this sum directly either due to size or representation of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math>, this constraint of only accessing the matrix through it’s vector product is familiar in numerical linear algebra.</p>
<p>An easy false start, for someone mainly experienced with eigenvalue problems, is to remember that the trace of a matrix is exactly the sum of the eigenvalues.
One would then ask, how can the eigenvalues be computed using only matrix-vector operations, and remember the Arnoldi iteration, an extension of the power method that constructs the Krylov subspace.
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>K</mi><mi>m</mi></msub><mo>=</mo><mrow><mo stretchy="true" form="prefix">[</mo><mi>x</mi><mo>,</mo><mi>A</mi><mi>x</mi><mo>,</mo><msup><mi>A</mi><mn>2</mn></msup><mi>x</mi><mo>,</mo><mi>…</mi><mo>,</mo><msup><mi>A</mi><mi>m</mi></msup><mi>x</mi><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex"> K_m = [x, A x, A^2 x, \dots, A^m x] </annotation></semantics></math>
After orthogonalizing the Krylov subspace with a Gram-Shmidt process into <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>Q</mi><mi>m</mi></msub><annotation encoding="application/x-tex">Q_m</annotation></semantics></math>, we can create an upper Hessenburg matrix <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>H</mi><mi>m</mi></msub><mo>=</mo><msubsup><mi>Q</mi><mi>m</mi><mo>*</mo></msubsup><mi>A</mi><msub><mi>Q</mi><mi>m</mi></msub></mrow><annotation encoding="application/x-tex">H_m = Q^*_m A Q_m</annotation></semantics></math>.
We can then apply QR to this smaller matrix ( <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mo>&lt;</mo><mo>&lt;</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">m &lt;&lt; n</annotation></semantics></math>) and use Rayleigh-Ritz to recover the associated eigenvalues of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math>.</p>
<p>Does this help us estimate the trace? Not really!
We most likely have recovered the eigenvalues of greatest magnitude (remember, power method), and to recover the rest of them, we either need to expand the Krylov subspace such that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mo>=</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">m = n</annotation></semantics></math>, which is totally intractable in this scenario, or we need to do restarted Arnoldi while shifting out converged eigenvalues, also likely intractable.
The usefulness of Arnoldi and other iterative eigenvalue algorithms is largely due to <em>not</em> needing all of the eigenvalues of these matrices.
This is not an entirely fruitless direction though, as the Rayleigh-Ritz procedure gives us a hint.</p>
<p>Let us recall the Rayleigh quotient, and how it can be written in terms of the eigenbasis <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>λ</mi><mi>i</mi></msub><mo>,</mo><msub><mi>v</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(\lambda_i, v_i)</annotation></semantics></math>.
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><msup><mi>x</mi><mo>*</mo></msup><mi>A</mi><mi>x</mi></mrow><mrow><msup><mi>x</mi><mo>*</mo></msup><mi>x</mi></mrow></mfrac><mo>=</mo><mfrac><mrow><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><msub><mi>λ</mi><mi>i</mi></msub><msup><mrow><mo stretchy="true" form="prefix">(</mo><msubsup><mi>v</mi><mi>i</mi><mo>*</mo></msubsup><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup></mrow><mrow><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><msup><mrow><mo stretchy="true" form="prefix">(</mo><msubsup><mi>v</mi><mi>i</mi><mo>*</mo></msubsup><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup></mrow></mfrac></mrow><annotation encoding="application/x-tex"> \frac{x^* A x}{x^* x} = \frac{\sum_{i=1}^N \lambda_i (v_i^* x)^2}{\sum_{i=1}^N (v_i^* x)^2} </annotation></semantics></math>
If we then choose <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math> such that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi><mrow><mo stretchy="true" form="prefix">[</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><msubsup><mi>v</mi><mi>i</mi><mo>*</mo></msubsup><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup><mo stretchy="true" form="postfix">]</mo></mrow><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">E[(v_i^* x)^2] = 1</annotation></semantics></math> we get (henceforth ignoring the denominator which is basically one-ish).
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi><mrow><mo stretchy="true" form="prefix">[</mo><msup><mi>x</mi><mo>*</mo></msup><mi>A</mi><mi>x</mi><mo stretchy="true" form="postfix">]</mo></mrow><mo>=</mo><mo>∑</mo><msub><mi>λ</mi><mi>i</mi></msub><mo>=</mo><mtext mathvariant="normal">tr</mtext><mi>A</mi></mrow><annotation encoding="application/x-tex"> E[x^* A x] = \sum \lambda_i = \text{tr} A </annotation></semantics></math>
We then use this to construct an estimator, using linearity.
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mtext mathvariant="normal">tr</mtext><mo accent="true">̂</mo></mover><mi>A</mi><mo>=</mo><mfrac><mn>1</mn><mi>M</mi></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>M</mi></munderover><msubsup><mi>x</mi><mi>i</mi><mo>*</mo></msubsup><mi>A</mi><msub><mi>x</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex"> \hat{\text{tr}} A = \frac{1}{M} \sum_{i=1}^M x_i^* A x_i </annotation></semantics></math></p>
<p>Let us take a detour and think how else we might come to this kind of conclusion.
What if we use indicator vectors <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>e</mi><mi>i</mi></msub><annotation encoding="application/x-tex">e_i</annotation></semantics></math> to recover the diagonal elements of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math>?
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">tr</mtext><mi>A</mi><mo>=</mo><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><msubsup><mi>e</mi><mi>i</mi><mi>T</mi></msubsup><mi>A</mi><msub><mi>e</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex"> \text{tr} A = \sum_{i=1}^N e_i^T A e_i  </annotation></semantics></math>
Given the constraints, it is unlikely doing <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> matrix-vector multiplications is feasible, but we can choose a random sampling of the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>e</mi><mi>i</mi></msub><annotation encoding="application/x-tex">e_i</annotation></semantics></math> for an approximate answer.</p>
<p>In light of sampling from the diagonal directly, our Arnoldi approach can be viewed as (biased) sampling from the diagonal in the eigenbasis, and the Rayleigh quotient approach is like sampling from the diagonal in a random basis.
All of the important analysis of this problem comes from the choice of distribution of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math>.</p>
<p>In the literature, the standard method is due to Hutchinson, where he chooses <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math> with elements sampled from the Rademacher distribution, that is, each element is <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>±</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\pm 1</annotation></semantics></math> with equal odds.
The general requirement for the estimator to be unbiased is <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi><mrow><mo stretchy="true" form="prefix">[</mo><mi>x</mi><msup><mi>x</mi><mo>*</mo></msup><mo stretchy="true" form="postfix">]</mo></mrow><mo>=</mo><mi>I</mi></mrow><annotation encoding="application/x-tex">E[x x^*] = I</annotation></semantics></math>, the expectation of the outer product being the identity. This is more general than our previous constraint.</p>
<p>Here is a simple implementation in Haskell using hmatrix.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">-- | Generate a random Rademacher vector (+1/-1 with equal probability)</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="ot">generateRademacherVector ::</span> <span class="dt">Int</span> <span class="ot">-&gt;</span> <span class="dt">IO</span> (<span class="dt">Vector</span> <span class="dt">R</span>)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>generateRademacherVector n <span class="ot">=</span> <span class="kw">do</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>    signs <span class="ot">&lt;-</span> replicateM n randomIO</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>    <span class="fu">return</span> <span class="op">$</span> vector <span class="op">$</span> <span class="fu">map</span> (\b <span class="ot">-&gt;</span> <span class="kw">if</span> b <span class="kw">then</span> <span class="dv">1</span> <span class="kw">else</span> <span class="op">-</span><span class="dv">1</span>) signs</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co">-- | Estimate the trace using the Girard-Hutchinson estimator</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="ot">estimateTrace ::</span> <span class="dt">Matrix</span> <span class="dt">R</span>   <span class="co">-- ^ Matrix</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>              <span class="ot">-&gt;</span> <span class="dt">Int</span>        <span class="co">-- ^ Matrix Dimension</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>              <span class="ot">-&gt;</span> <span class="dt">Int</span>        <span class="co">-- ^ Number of samples</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>              <span class="ot">-&gt;</span> <span class="dt">IO</span> <span class="dt">R</span>       <span class="co">-- ^ Estimated trace</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>estimateTrace a n numSamples <span class="ot">=</span> <span class="kw">do</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>    <span class="co">-- Generate random vectors and compute estimates</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>    estimates <span class="ot">&lt;-</span> replicateM numSamples <span class="op">$</span> <span class="kw">do</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>        v <span class="ot">&lt;-</span> generateRademacherVector n</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>        <span class="co">-- Rayleigh quotient: we can only access a through mat-vec</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>        <span class="fu">return</span> <span class="op">$</span> v <span class="op">&lt;.&gt;</span> (a <span class="op">#&gt;</span> v)</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>    <span class="co">-- Average the estimates</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>    <span class="fu">return</span> <span class="op">$</span> <span class="fu">sum</span> estimates <span class="op">/</span> <span class="fu">fromIntegral</span> numSamples</span></code></pre></div>
<p>The actual performance of this algorithm, and the correct choice of distribution of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math> is heavily dependent on the structure of the matrix.
If the density of the trace is heavily concentrated, for example one value of 100 and the rest zeros, we would expect indicator vectors to perform very poorly!
In general, through some contortion of the central limit theorem, we should expect convergence to be on the order of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msqrt><mi>n</mi></msqrt><annotation encoding="application/x-tex">\sqrt{n}</annotation></semantics></math> where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math> is the number of samples, which is not very fast, but good enough in practice for anything that might warrant such an approximation the begin with.</p>
    </section>
  </article>
</main>
]]></description>
    <pubDate>Sat, 11 Jan 2025 00:00:00 UT</pubDate>
    <guid>https://spacedome.tv/posts/trace-estimation.html</guid>
    <dc:creator>Julien</dc:creator>
</item>
<item>
    <title>FFT in Haskell and Futhark</title>
    <link>https://spacedome.tv/posts/fft-in-haskell-and-futhark.html</link>
    <description><![CDATA[<main>
  <article>
    <header>
    <h1>FFT in Haskell and Futhark</h1>
    <p><i>2024-12-24 (updated: 2024-12-24 11:11)</i></p>
    <hr/>
    </header>
    <section>
      <p>The Fourier transform is one of the fundamental tools in analysis.
From the perspective of approximation theory, it gives us one of the orthogonal function bases, the natural basis for periodic functions.
To use this numerically, as with any basis, we must sample from the function to be approximated, and periodic functions have a wonderful property that the optimal points to sample are uniformly spaced on the interval.
This is very different from polynomial bases like Chebyshev or Legendre polynomials, where choosing the points is somewhat involved.
For Fourier, this leads us to the Discrete Fourier Transform (DFT), a cornerstone of signal processing.</p>
<p>Let <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>x</mi><mi>n</mi></msub><annotation encoding="application/x-tex">x_n</annotation></semantics></math> be our signal sampled at <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> points, i.e. <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math> is a sequence of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> real or complex numbers.
Let <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ω</mi><mo>=</mo><msub><mi>ω</mi><mi>N</mi></msub><mo>=</mo><msup><mi>e</mi><mrow><mo>−</mo><mn>2</mn><mi>π</mi><mi>i</mi><mi>/</mi><mi>N</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\omega = \omega_N = e^{-2\pi i / N}</annotation></semantics></math> be the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>N</mi><mtext mathvariant="normal">th</mtext></msup><annotation encoding="application/x-tex">N^\text{th}</annotation></semantics></math> root of unity, the powers of which are sometimes called “twiddle” factors in this context.
We can then define the DFT element-wise as follows.</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mi>t</mi></msub><mo>=</mo><munderover><mo>∑</mo><mrow><mi>n</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>N</mi><mo>−</mo><mn>1</mn></mrow></munderover><msub><mi>x</mi><mi>n</mi></msub><mo>⋅</mo><msup><mi>ω</mi><mrow><mi>t</mi><mi>n</mi></mrow></msup></mrow><annotation encoding="application/x-tex"> y_t =\sum_{n=0}^{N-1} x_{n} \cdot \omega^{t n} </annotation></semantics></math></p>
<p>We see that each element is the dot product of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math> with a vector of the twiddle factors, so we can represent this as a matrix multiplication <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>=</mo><msub><mi>W</mi><mi>N</mi></msub><mi>x</mi></mrow><annotation encoding="application/x-tex">y = W_N x</annotation></semantics></math> where</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mi>N</mi></msub><mo>=</mo><mrow><mo stretchy="true" form="prefix">[</mo><mtable><mtr><mtd columnalign="center"><msubsup><mi>ω</mi><mi>N</mi><mn>0</mn></msubsup></mtd><mtd columnalign="center"><msubsup><mi>ω</mi><mi>N</mi><mn>0</mn></msubsup></mtd><mtd columnalign="center"><msubsup><mi>ω</mi><mi>N</mi><mn>0</mn></msubsup></mtd><mtd columnalign="center"><mi>⋯</mi></mtd><mtd columnalign="center"><msubsup><mi>ω</mi><mi>N</mi><mn>0</mn></msubsup></mtd></mtr><mtr><mtd columnalign="center"><msubsup><mi>ω</mi><mi>N</mi><mn>0</mn></msubsup></mtd><mtd columnalign="center"><msubsup><mi>ω</mi><mi>N</mi><mn>1</mn></msubsup></mtd><mtd columnalign="center"><msubsup><mi>ω</mi><mi>N</mi><mn>2</mn></msubsup></mtd><mtd columnalign="center"><mi>⋯</mi></mtd><mtd columnalign="center"><msubsup><mi>ω</mi><mi>N</mi><mrow><mi>N</mi><mo>−</mo><mn>1</mn></mrow></msubsup></mtd></mtr><mtr><mtd columnalign="center"><msubsup><mi>ω</mi><mi>N</mi><mn>0</mn></msubsup></mtd><mtd columnalign="center"><msubsup><mi>ω</mi><mi>N</mi><mn>2</mn></msubsup></mtd><mtd columnalign="center"><msubsup><mi>ω</mi><mi>N</mi><mn>4</mn></msubsup></mtd><mtd columnalign="center"><mi>⋯</mi></mtd><mtd columnalign="center"><msubsup><mi>ω</mi><mi>N</mi><mrow><mn>2</mn><mrow><mo stretchy="true" form="prefix">(</mo><mi>N</mi><mo>−</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow></msubsup></mtd></mtr><mtr><mtd columnalign="center"><mi>⋮</mi></mtd><mtd columnalign="center"><mi>⋮</mi></mtd><mtd columnalign="center"><mi>⋮</mi></mtd><mtd columnalign="center"><mo>⋱</mo></mtd><mtd columnalign="center"><mi>⋮</mi></mtd></mtr><mtr><mtd columnalign="center"><msubsup><mi>ω</mi><mi>N</mi><mn>0</mn></msubsup></mtd><mtd columnalign="center"><msubsup><mi>ω</mi><mi>N</mi><mrow><mi>N</mi><mo>−</mo><mn>1</mn></mrow></msubsup></mtd><mtd columnalign="center"><msubsup><mi>ω</mi><mi>N</mi><mrow><mn>2</mn><mrow><mo stretchy="true" form="prefix">(</mo><mi>N</mi><mo>−</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow></msubsup></mtd><mtd columnalign="center"><mi>⋯</mi></mtd><mtd columnalign="center"><msubsup><mi>ω</mi><mi>N</mi><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>N</mi><mo>−</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>N</mi><mo>−</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow></msubsup></mtd></mtr></mtable><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex"> 
W_N = \begin{bmatrix} 
\omega^0_N &amp; \omega^0_N &amp; \omega^0_N &amp; \cdots &amp; \omega^0_N \\
\omega^0_N &amp; \omega_N^1 &amp; \omega_N^2 &amp; \cdots &amp; \omega_N^{N-1} \\
\omega^0_N &amp; \omega_N^2 &amp; \omega_N^4 &amp; \cdots &amp; \omega_N^{2(N-1)} \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\omega^0_N &amp; \omega_N^{N-1} &amp; \omega_N^{2(N-1)} &amp; \cdots &amp; \omega_N^{(N-1)(N-1)}
\end{bmatrix}
</annotation></semantics></math></p>
<p>We will also denote this as <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>=</mo><mi>F</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">y = F(x)</annotation></semantics></math>.
The complexity of matrix-vector multiplication is trivially <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>n</mi><mn>2</mn></msup><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(n^2)</annotation></semantics></math>, as we must access every element of the matrix.
In the 1960s, during the great boom of numerical research, James Cooley and John Tukey published their paper exploiting the structure of the matrix for a log-linear solution, ushering in the age of real time digital signal processing.
This family of algorithms is called the Fast Fourier Transform (FFT).</p>
<p>The key insight of the Cooley-Tukey FFT is that one can split the signal in half and recursively compute the FFT.
This is one of the early examples of divide and conquer algorithms, along with merge sort, which shares a similar <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>log</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">n \log n</annotation></semantics></math> time complexity.
For simplicity we assume <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> is a power of two, so that we can divide in half until we get to a single element, though this is not necessary with real FFT implementations.
Recursive algorithms are often most naturally expressed in functional languages, so we derive a recursive form to implement in Haskell.</p>
<p>First we identify the base case, which is simply the identity <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>=</mo><msup><mi>ω</mi><mn>0</mn></msup><mi>x</mi><mo>=</mo><mi>x</mi></mrow><annotation encoding="application/x-tex">y = \omega^0 x = x</annotation></semantics></math>.
For <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>=</mo><mn>2</mn><mo>,</mo><mi>N</mi><mo>=</mo><mn>4</mn></mrow><annotation encoding="application/x-tex">N = 2, N=4</annotation></semantics></math> it is instructive to do examples out by hand in full using the matrix multiplication <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mi>N</mi></msub><mi>x</mi></mrow><annotation encoding="application/x-tex">W_N x</annotation></semantics></math>.
For <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>=</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">N = 2</annotation></semantics></math> we get the following.</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right"><msub><mi>y</mi><mn>0</mn></msub></mtd><mtd columnalign="left"><mo>=</mo><msub><mi>x</mi><mn>0</mn></msub><mo>+</mo><msub><mi>x</mi><mn>1</mn></msub></mtd></mtr><mtr><mtd columnalign="right"><msub><mi>y</mi><mn>1</mn></msub></mtd><mtd columnalign="left"><mo>=</mo><msub><mi>x</mi><mn>0</mn></msub><mo>−</mo><msub><mi>x</mi><mn>1</mn></msub></mtd></mtr></mtable><annotation encoding="application/x-tex"> \begin{align}  y_0 &amp;= x_0 + x_1 \\ y_1 &amp;= x_0 - x_1 \end{align} </annotation></semantics></math></p>
<p>When drawn out as a data flow diagram, as you would see in more hardware-adjacent expositions, this forms a cross-over, leading to the name <a href="https://en.wikipedia.org/wiki/Butterfly_diagram">butterfly</a> for the combining stage of the FFT.</p>
<p>The trick to the recursion is that splitting <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math> into even <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>x</mi><mi>e</mi></msup><mo>=</mo><mo stretchy="false" form="prefix">{</mo><msub><mi>x</mi><mn>0</mn></msub><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo>,</mo><mi>.</mi><mi>.</mi><mi>.</mi><mo stretchy="false" form="postfix">}</mo></mrow><annotation encoding="application/x-tex">x^e = \{x_0, x_2, ...\}</annotation></semantics></math> and odd <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>x</mi><mi>o</mi></msup><mo>=</mo><mo stretchy="false" form="prefix">{</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><mn>3</mn></msub><mo>,</mo><mi>.</mi><mi>.</mi><mi>.</mi><mo stretchy="false" form="postfix">}</mo></mrow><annotation encoding="application/x-tex">x^o = \{x_1, x_3, ...\}</annotation></semantics></math> components.
This can be seen by rewriting the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>=</mo><mn>4</mn></mrow><annotation encoding="application/x-tex">N=4</annotation></semantics></math> case out by hand, I will leave out the derivation here, but the result should look like the following.</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi><mo>=</mo><mi>F</mi><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>x</mi><mi>e</mi></msup><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msub><mi>W</mi><mn>2</mn></msub><mrow><mo stretchy="true" form="prefix">[</mo><mtable><mtr><mtd columnalign="center"><msub><mi>x</mi><mn>0</mn></msub></mtd></mtr><mtr><mtd columnalign="center"><msub><mi>x</mi><mn>2</mn></msub></mtd></mtr></mtable><mo stretchy="true" form="postfix">]</mo></mrow><mo>,</mo><mspace width="1.0em"></mspace><mi>v</mi><mo>=</mo><mi>F</mi><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>x</mi><mi>o</mi></msup><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msub><mi>W</mi><mn>2</mn></msub><mrow><mo stretchy="true" form="prefix">[</mo><mtable><mtr><mtd columnalign="center"><msub><mi>x</mi><mn>1</mn></msub></mtd></mtr><mtr><mtd columnalign="center"><msub><mi>x</mi><mn>3</mn></msub></mtd></mtr></mtable><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex"> 
u = F(x^e) = W_2 \begin{bmatrix} x_0 \\ x_2  \end{bmatrix}, \quad
v = F(x^o) = W_2 \begin{bmatrix} x_1 \\ x_3  \end{bmatrix}
</annotation></semantics></math>
We then combine the two sub-problems.
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>=</mo><mi>F</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mrow><mo stretchy="true" form="prefix">[</mo><mtable><mtr><mtd columnalign="center"><msub><mi>u</mi><mn>0</mn></msub><mo>+</mo><msup><mi>ω</mi><mn>0</mn></msup><msub><mi>v</mi><mn>0</mn></msub></mtd></mtr><mtr><mtd columnalign="center"><msub><mi>u</mi><mn>1</mn></msub><mo>+</mo><msup><mi>ω</mi><mn>1</mn></msup><msub><mi>v</mi><mn>1</mn></msub></mtd></mtr><mtr><mtd columnalign="center"><msub><mi>u</mi><mn>0</mn></msub><mo>+</mo><msup><mi>ω</mi><mn>2</mn></msup><msub><mi>v</mi><mn>0</mn></msub></mtd></mtr><mtr><mtd columnalign="center"><msub><mi>u</mi><mn>1</mn></msub><mo>+</mo><msup><mi>ω</mi><mn>3</mn></msup><msub><mi>v</mi><mn>1</mn></msub></mtd></mtr></mtable><mo stretchy="true" form="postfix">]</mo></mrow><mo>=</mo><mrow><mo stretchy="true" form="prefix">[</mo><mtable><mtr><mtd columnalign="center"><msub><mi>u</mi><mn>0</mn></msub><mo>+</mo><msup><mi>ω</mi><mn>0</mn></msup><msub><mi>v</mi><mn>0</mn></msub></mtd></mtr><mtr><mtd columnalign="center"><msub><mi>u</mi><mn>1</mn></msub><mo>+</mo><msup><mi>ω</mi><mn>1</mn></msup><msub><mi>v</mi><mn>1</mn></msub></mtd></mtr><mtr><mtd columnalign="center"><msub><mi>u</mi><mn>0</mn></msub><mo>−</mo><msup><mi>ω</mi><mn>0</mn></msup><msub><mi>v</mi><mn>0</mn></msub></mtd></mtr><mtr><mtd columnalign="center"><msub><mi>u</mi><mn>1</mn></msub><mo>−</mo><msup><mi>ω</mi><mn>1</mn></msup><msub><mi>v</mi><mn>1</mn></msub></mtd></mtr></mtable><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex"> 
y = F(x) = \begin{bmatrix}
u_0 + \omega^0 v_0 \\
u_1 + \omega^1 v_1 \\
u_0 + \omega^2 v_0 \\
u_1 + \omega^3 v_1
\end{bmatrix} = \begin{bmatrix}
u_0 + \omega^0 v_0 \\
u_1 + \omega^1 v_1 \\
u_0 - \omega^0 v_0 \\
u_1 - \omega^1 v_1
\end{bmatrix}
</annotation></semantics></math>
This is not entirely intuitive and I encourage you to look in an introductory numerical analysis textbook if you would like to be guided through the derivation.
Note that the last equality is just using <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>ω</mi><mi>N</mi><mrow><mi>N</mi><mi>/</mi><mn>2</mn></mrow></msubsup><mo>=</mo><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\omega_N^{N/2} = -1</annotation></semantics></math> to simplify, this is very helpful computationally, as the bottom half and top half of the vector are now much more similar.
From this we have the motivation for the recursive definition we will implement.
Let <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi><mo>=</mo><mi>F</mi><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>x</mi><mi>e</mi></msup><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><mi>v</mi><mo>=</mo><mi>F</mi><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>x</mi><mi>o</mi></msup><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">u = F(x^e), v = F(x^o)</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">[</mo><msup><mi>ω</mi><mn>0</mn></msup><mo>,</mo><mi>.</mi><mi>.</mi><mi>.</mi><mo>,</mo><msup><mi>ω</mi><mrow><mi>N</mi><mi>/</mi><mn>2</mn><mo>−</mo><mn>1</mn></mrow></msup><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">T = [\omega^0, ..., \omega^{N/2-1}]</annotation></semantics></math> be a vector of twiddle factors, with <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mo>⊙</mo><annotation encoding="application/x-tex">\odot</annotation></semantics></math> being element-wise “broadcasting” multiplication.
Then we can derive the following, abusing matrix notation somewhat.
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">[</mo><mtable><mtr><mtd columnalign="center"><mi>u</mi><mo>+</mo><mi>T</mi><mo>⊙</mo><mi>v</mi></mtd></mtr><mtr><mtd columnalign="center"><mi>u</mi><mo>−</mo><mi>T</mi><mo>⊙</mo><mi>v</mi></mtd></mtr></mtable><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">
y = \begin{bmatrix}
u + T \odot v \\
u - T \odot v
\end{bmatrix}
</annotation></semantics></math></p>
<p>A minimal Haskell implementation of this recursive form is quite elegant.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="ot">split ::</span> [a] <span class="ot">-&gt;</span> ([a], [a])</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>split [] <span class="ot">=</span> ([], [])</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>split [_] <span class="ot">=</span> <span class="fu">error</span> <span class="st">&quot;input size must be power of two&quot;</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>split (x<span class="op">:</span>y<span class="op">:</span>xs) <span class="ot">=</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>  <span class="kw">let</span> (es, os) <span class="ot">=</span> split xs</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>  <span class="kw">in</span> (x<span class="op">:</span>es, y<span class="op">:</span>os)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="ot">mergeRadix2 ::</span> [<span class="dt">Complex</span> <span class="dt">Double</span>] <span class="ot">-&gt;</span> [<span class="dt">Complex</span> <span class="dt">Double</span>] <span class="ot">-&gt;</span> <span class="dt">Int</span> <span class="ot">-&gt;</span> [<span class="dt">Complex</span> <span class="dt">Double</span>]</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>mergeRadix2 u v n <span class="ot">=</span> (<span class="op">++</span>) (<span class="fu">zipWith</span> (<span class="op">+</span>) u q) (<span class="fu">zipWith</span> (<span class="op">-</span>) u q)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>  <span class="kw">where</span> q <span class="ot">=</span> <span class="fu">zipWith</span> (<span class="op">*</span>) v w</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>        n2 <span class="ot">=</span> <span class="fu">length</span> u <span class="op">-</span> <span class="dv">1</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>        w <span class="ot">=</span> [<span class="fu">exp</span> (<span class="dv">0</span> <span class="op">:+</span> (<span class="op">-</span><span class="dv">2</span> <span class="op">*</span> <span class="fu">pi</span> <span class="op">*</span> <span class="fu">fromIntegral</span> k <span class="op">/</span> <span class="fu">fromIntegral</span> n )) <span class="op">|</span> k <span class="ot">&lt;-</span> [<span class="dv">0</span><span class="op">..</span>n2]]</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="ot">fft ::</span> [<span class="dt">Complex</span> <span class="dt">Double</span>] <span class="ot">-&gt;</span> [<span class="dt">Complex</span> <span class="dt">Double</span>]</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>fft [] <span class="ot">=</span> []</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>fft [z] <span class="ot">=</span> [z]</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>fft zs <span class="ot">=</span> mergeRadix2 (fft evens) (fft odds) (<span class="fu">length</span> zs)</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>  <span class="kw">where</span> (evens, odds) <span class="ot">=</span> split zs</span></code></pre></div>
<p>One might immediately ask about performance, and yes, this implementation is meant only to be instructive, but explicitly recursive implementations can be competitive.
The first place to look is FFTW, the state of the art software FFT library, which takes a “bag of algorithms + planner” approach.
It is implemented with OCaml for code generation with many passes of optimization to create a portable C library, and many of the variants are recursive.</p>
<p>The obvious suspects in a numerical algorithm optimization such as this are:</p>
<ul>
<li>Avoiding memory reallocation and optimizing cache locality.</li>
<li>Using lookup tables or otherwise avoiding trigonometric calculation.</li>
</ul>
<h2 id="implementing-the-fft-in-futhark">Implementing the FFT in Futhark</h2>
<p>I wanted to try Futhark, the pure functional array based language implemented in Haskell that compiles to C or Cuda/OpenCL, and thought this algorithm would be a good fit.
There is a Stockham variant in the Futhark packages for reference, but I implemented Cooley-Tukey Radix-2.
Unfortunately Futhark does not support explicit recursion, and it is not clear (to me at least) if it ever will.
My understanding is that it may be possible in the future, though there are fundamental difficulties, as the stack cannot be used willy-nilly on a GPU, so any recursion would be limited in nature, and currently you just have to unroll it into a loop manually.
This means we cannot implement a recursive FFT, but must do the more complicated iterative approach.</p>
<p>I attempted to use Claude for this, to see how it would do with a relative obscure programming language, surprisingly it mostly worked, though it consistently would get indexing wrong and mostly would not use the array combinators correctly.
The main points of the iterative approach are that successive applications of the even/odd splits can be viewed as a rearrangement by “bit reversal permutation” and that we must do much tedious indexing to keep track of the arithmetic combinations, these are the “butterflies” previously mentioned.
Not going into depth, here is my implementation.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>def twiddle (k<span class="op">:</span> i64) (n<span class="op">:</span> i64)<span class="op">:</span> complex <span class="ot">=</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="kw">let</span> angle <span class="ot">=</span> <span class="op">-</span><span class="fl">2.0</span> <span class="op">*</span> f64<span class="op">.</span><span class="fu">pi</span> <span class="op">*</span> f64<span class="op">.</span>i64 k <span class="op">/</span> f64<span class="op">.</span>i64 n</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="kw">in</span> (f64<span class="op">.</span><span class="fu">cos</span> angle, f64<span class="op">.</span><span class="fu">sin</span> angle)</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>def bit_reversal [n] &#39;t (input<span class="op">:</span> [n]t)<span class="op">:</span> [n]t <span class="ot">=</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>  <span class="kw">let</span> bits <span class="ot">=</span> i64<span class="op">.</span>f64 (f64<span class="op">.</span>log2 (f64<span class="op">.</span>i64 n))</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>  <span class="kw">let</span> indices <span class="ot">=</span> <span class="fu">map</span> (\i <span class="ot">-&gt;</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">let</span> rev <span class="ot">=</span> loop rev <span class="ot">=</span> <span class="dv">0</span> for j <span class="op">&lt;</span> bits <span class="kw">do</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>      (rev <span class="op">&lt;&lt;</span> <span class="dv">1</span>) <span class="op">|</span> ((i <span class="op">&gt;&gt;</span> j) <span class="op">&amp;</span> <span class="dv">1</span>)</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">in</span> rev</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>  ) (iota n)</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>  <span class="kw">in</span> spread n (input[<span class="dv">0</span>]) indices input</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a><span class="co">-- Type to hold butterfly operation parameters</span></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a><span class="kw">type</span> butterfly_params <span class="ot">=</span> {</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>  upper_idx<span class="op">:</span> i64,    <span class="co">-- Index of upper butterfly input</span></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>  lower_idx<span class="op">:</span> i64,    <span class="co">-- Index of lower butterfly input</span></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>  twiddle<span class="op">:</span> complex   <span class="co">-- Twiddle factor for this butterfly</span></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a><span class="co">-- Calculate butterfly parameters for a given stage</span></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>def get_butterfly_params (stage<span class="op">:</span> i64) (n<span class="op">:</span> i64) (i<span class="op">:</span> i64)<span class="op">:</span> butterfly_params <span class="ot">=</span></span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>  <span class="kw">let</span> butterfly_size <span class="ot">=</span> <span class="dv">1</span> <span class="op">&lt;&lt;</span> (stage <span class="op">+</span> <span class="dv">1</span>)        <span class="co">-- Size of entire butterfly</span></span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>  <span class="kw">let</span> half_size <span class="ot">=</span> butterfly_size <span class="op">&gt;&gt;</span> <span class="dv">1</span>          <span class="co">-- Size of half butterfly</span></span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>  <span class="kw">let</span> <span class="fu">group</span> <span class="ot">=</span> i <span class="op">/</span> butterfly_size               <span class="co">-- Which group of butterflies</span></span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>  <span class="kw">let</span> k <span class="ot">=</span> i <span class="op">%</span> half_size                        <span class="co">-- Position within half</span></span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>  <span class="kw">let</span> group_start <span class="ot">=</span> <span class="fu">group</span> <span class="op">*</span> butterfly_size     <span class="co">-- Start index of this group</span></span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a>  <span class="kw">let</span> twiddle_idx <span class="ot">=</span> k <span class="op">*</span> (n <span class="op">/</span> butterfly_size)   <span class="co">-- Index for twiddle factor</span></span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a>  <span class="kw">in</span> {</span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>    upper_idx <span class="ot">=</span> group_start <span class="op">+</span> k,</span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>    lower_idx <span class="ot">=</span> group_start <span class="op">+</span> k <span class="op">+</span> half_size,</span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a>    twiddle <span class="ot">=</span> twiddle twiddle_idx n</span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a><span class="co">-- Perform single butterfly operation</span></span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a>def butterfly_op (<span class="kw">data</span><span class="op">:</span> []complex) (p<span class="op">:</span> butterfly_params) (is_upper<span class="op">:</span> bool)<span class="op">:</span> complex <span class="ot">=</span></span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a>  <span class="kw">if</span> is_upper</span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a>  <span class="kw">then</span> complex_add <span class="kw">data</span>[p<span class="op">.</span>upper_idx]</span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a>                  (complex_mul <span class="kw">data</span>[p<span class="op">.</span>lower_idx] p<span class="op">.</span>twiddle)</span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a>  <span class="kw">else</span> complex_sub <span class="kw">data</span>[p<span class="op">.</span>upper_idx]</span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a>                  (complex_mul <span class="kw">data</span>[p<span class="op">.</span>lower_idx] p<span class="op">.</span>twiddle)</span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-43"><a href="#cb2-43" aria-hidden="true" tabindex="-1"></a><span class="co">-- Main FFT function</span></span>
<span id="cb2-44"><a href="#cb2-44" aria-hidden="true" tabindex="-1"></a>def fft [n] (input<span class="op">:</span> [n]complex)<span class="op">:</span> [n]complex <span class="ot">=</span></span>
<span id="cb2-45"><a href="#cb2-45" aria-hidden="true" tabindex="-1"></a>  <span class="kw">let</span> bits <span class="ot">=</span> i64<span class="op">.</span>f64 (f64<span class="op">.</span>log2 (f64<span class="op">.</span>i64 n))</span>
<span id="cb2-46"><a href="#cb2-46" aria-hidden="true" tabindex="-1"></a>  <span class="co">-- This method can only handle arrays of length 2^n</span></span>
<span id="cb2-47"><a href="#cb2-47" aria-hidden="true" tabindex="-1"></a>  <span class="kw">in</span> assert (n <span class="op">==</span> <span class="dv">1</span> <span class="op">&lt;&lt;</span> bits) (</span>
<span id="cb2-48"><a href="#cb2-48" aria-hidden="true" tabindex="-1"></a>    <span class="co">-- First apply bit reversal permutation</span></span>
<span id="cb2-49"><a href="#cb2-49" aria-hidden="true" tabindex="-1"></a>    <span class="kw">let</span> reordered <span class="ot">=</span> bit_reversal input</span>
<span id="cb2-50"><a href="#cb2-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-51"><a href="#cb2-51" aria-hidden="true" tabindex="-1"></a>    <span class="co">-- Perform log2(n) stages of butterfly operations</span></span>
<span id="cb2-52"><a href="#cb2-52" aria-hidden="true" tabindex="-1"></a>    <span class="kw">in</span> loop <span class="kw">data</span> <span class="ot">=</span> reordered for stage <span class="op">&lt;</span> bits <span class="kw">do</span></span>
<span id="cb2-53"><a href="#cb2-53" aria-hidden="true" tabindex="-1"></a>      <span class="co">-- For each stage, compute butterfly parameters and perform operations</span></span>
<span id="cb2-54"><a href="#cb2-54" aria-hidden="true" tabindex="-1"></a>      <span class="kw">let</span> butterfly_size <span class="ot">=</span> <span class="dv">1</span> <span class="op">&lt;&lt;</span> (stage <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb2-55"><a href="#cb2-55" aria-hidden="true" tabindex="-1"></a>      <span class="kw">let</span> half_size <span class="ot">=</span> butterfly_size <span class="op">&gt;&gt;</span> <span class="dv">1</span></span>
<span id="cb2-56"><a href="#cb2-56" aria-hidden="true" tabindex="-1"></a>      <span class="kw">let</span> params <span class="ot">=</span> <span class="fu">map</span> (get_butterfly_params stage n) (iota n)</span>
<span id="cb2-57"><a href="#cb2-57" aria-hidden="true" tabindex="-1"></a>      <span class="kw">in</span> map2 (\p i <span class="ot">-&gt;</span></span>
<span id="cb2-58"><a href="#cb2-58" aria-hidden="true" tabindex="-1"></a>        <span class="kw">let</span> is_upper <span class="ot">=</span> (i <span class="op">%</span> butterfly_size) <span class="op">&lt;</span> half_size</span>
<span id="cb2-59"><a href="#cb2-59" aria-hidden="true" tabindex="-1"></a>        <span class="kw">in</span> butterfly_op <span class="kw">data</span> p is_upper</span>
<span id="cb2-60"><a href="#cb2-60" aria-hidden="true" tabindex="-1"></a>      ) params (iota n)</span>
<span id="cb2-61"><a href="#cb2-61" aria-hidden="true" tabindex="-1"></a>  )</span></code></pre></div>
<p>This is not particularly optimized. Futhark allows for fused memory operations and has a semantics for tracking when it is safe to overwrite memory while remaining pure, I did not use this here. I did make sure to use <code>spread</code> and <code>map2</code> array combinators when traversing, which theoretically should allow for some automatic parallelism, though I did not test this, as I don’t have CUDA running on my laptop.</p>
<p>Futhark is slowly emerging from being an academic project into a serious tool, and the ecosystem is still in its infancy.
I wanted to try implementing some of my research in eigensolvers, but the linear algebra module is at the level of undergraduate research project, and does not appear to support complex matrices at the moment.
Personally, I probably will not use it further at the moment, but it is very much the direction I would like numerical algorithms to go, with functional DSLs (or full languages) that compile to highly portable, highly optimized code.</p>
    </section>
  </article>
</main>
]]></description>
    <pubDate>Tue, 24 Dec 2024 00:00:00 UT</pubDate>
    <guid>https://spacedome.tv/posts/fft-in-haskell-and-futhark.html</guid>
    <dc:creator>Julien</dc:creator>
</item>
<item>
    <title>Moving from Hugo to Hakyll</title>
    <link>https://spacedome.tv/posts/moving-from-hugo-to-hakyll.html</link>
    <description><![CDATA[<main>
  <article>
    <header>
    <h1>Moving from Hugo to Hakyll</h1>
    <p><i>2024-11-09 (updated: 2024-11-09 10:59)</i></p>
    <hr/>
    </header>
    <section>
      <p>Five years ago I decided to rewrite this website with a static site generator, and chose Hugo on a whim.
After letting it sit around unused for far too long, I moved it to <a href="https://jaspervdj.be/hakyll/">Hakyll</a>.
Now that I’m getting back into Haskell, it was a perfect small project, and Pandoc is unbeatable as a document conversion backend.</p>
<p>The built in code highlighter works well enough if you override some of the styling.
It is aparently also possible to use pygmentize if you want to go through the trouble.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="ot">toSlug ::</span> <span class="dt">T.Text</span> <span class="ot">-&gt;</span> <span class="dt">T.Text</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>toSlug <span class="ot">=</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>  T.intercalate (T.singleton <span class="ch">&#39;-&#39;</span>) <span class="op">.</span> T.words <span class="op">.</span> T.toLower <span class="op">.</span> clean</span></code></pre></div>
<p>I chose a perfect time to move, as MathML finally has full support across all browsers as of Chromium 109, so the whole MathJax/KaTeX and JS rendering question is finally over.
We are free!
Here is an example and the associated markdown:</p>
<p>Inline math with MathML: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>=</mo><mi>m</mi><mi>x</mi><mo>+</mo><mi>b</mi></mrow><annotation encoding="application/x-tex">y = mx +b</annotation></semantics></math>.
Block/display style:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>ln</mo><mi>x</mi><mo>=</mo><msubsup><mo>∫</mo><mrow><mo>−</mo><mi>∞</mi></mrow><mi>x</mi></msubsup><mfrac><mn>1</mn><mi>y</mi></mfrac><mspace width="0.167em"></mspace><mstyle mathvariant="normal"><mi>d</mi></mstyle><mi>y</mi><mi>.</mi></mrow><annotation encoding="application/x-tex"> \ln x = \int_{-\infty}^x \frac 1 y \, \mathrm{d}y . </annotation></semantics></math></p>
<pre class="text"><code>Inline math with MathML: \\( y = mx +b \\). Block/display style:
\\[ \ln x = \int_{-\infty}^x \frac 1 y \, \mathrm{d}y . \\]</code></pre>
<p>This is as easy as enabling the <code>Ext_tex_math_double_backslash</code> Pandoc extension and setting <code>writerHTMLMathMethod = MathML</code> in the Pandoc writer options.
It is significantly uglier than what TeX would give you, but this is the price you pay if you want to avoid javascript or prerendering SVGs.</p>
<p>If you do want your math to look nice, instead of using the MathJax Pandoc writer, you can use MathML and put the MathJax CDN script in your header for drop in MathJax rendering that falls back gracefully to MathML instead of the typical raw LaTeX text.
Why this isn’t the default behavior of the MathJax writer in Pandoc, who knows.
You can also pre-render with KaTeX to get pretty HTML results, but this involves running javascript during the build process.</p>
<p>To see what this page looks like redered as pure MathML, just disable javascript.
Thankfully, while still a real Hog coming in around 250kB compressed, MathJax 3 is much faster than it used to be.
Definitely make sure to have this included only on the necessary pages.</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow><mo stretchy="true" form="prefix">(</mo><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msub><mi>a</mi><mi>k</mi></msub><msub><mi>b</mi><mi>k</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup><mo>≤</mo><mrow><mo stretchy="true" form="prefix">(</mo><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msubsup><mi>a</mi><mi>k</mi><mn>2</mn></msubsup><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msubsup><mi>b</mi><mi>k</mi><mn>2</mn></msubsup><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\left( \sum_{k=1}^n a_k b_k \right)^2 \leq \left( \sum_{k=1}^n a_k^2 \right) \left( \sum_{k=1}^n b_k^2 \right) </annotation></semantics></math></p>
    </section>
  </article>
</main>
]]></description>
    <pubDate>Sat, 09 Nov 2024 00:00:00 UT</pubDate>
    <guid>https://spacedome.tv/posts/moving-from-hugo-to-hakyll.html</guid>
    <dc:creator>Julien</dc:creator>
</item>
<item>
    <title>Cross-Validation Methodology in Materials Science</title>
    <link>https://spacedome.tv/posts/cross-validation-methodology-in-materials-science.html</link>
    <description><![CDATA[<main>
  <article>
    <header>
    <h1>Cross-Validation Methodology in Materials Science</h1>
    <p><i>2018-09-01 </i></p>
    <hr/>
    </header>
    <section>
      <p>Cross-validation is a critical part of statistical methodology, for ad hoc models cross-validation may be the only indication of model performance, and without a reasonable cross-validation methodology serious over-fitting can go undetected.
This issue is particularly relevant to domains where small data-sets with a comparatively large number of features is common, for example Materials Science or Genomics.
If the cross-validation method does not take into consideration the feature selection (that is, considering feature selection as part of model selection), a significant selection bias can occur, see <a href="https://doi.org/10.1073/pnas.102102699">this paper</a> for an example with gene-expression data.
In general, a reasonably robust validation methodology should be chosen before model selection, and final hold-out sets should be used when possible.</p>
<h1 id="poster-pdf"><a href="/pdf/poster_SULI.pdf">Poster PDF</a></h1>
    </section>
  </article>
</main>
]]></description>
    <pubDate>Sat, 01 Sep 2018 00:00:00 UT</pubDate>
    <guid>https://spacedome.tv/posts/cross-validation-methodology-in-materials-science.html</guid>
    <dc:creator>Julien</dc:creator>
</item>
<item>
    <title>Learning Hyper-Parameters with Bayesian Optimization</title>
    <link>https://spacedome.tv/posts/learning-hyper-parameters-with-bayesian-optimization.html</link>
    <description><![CDATA[<main>
  <article>
    <header>
    <h1>Learning Hyper-Parameters with Bayesian Optimization</h1>
    <p><i>2018-05-01 </i></p>
    <hr/>
    </header>
    <section>
      <p>It is well known in the Machine Learning literature that a Grid Search for optimizing parameters is sub-optimal, and generally outperformed by a Random Search.
This project explored using Bayesian Optimization as a sequential search strategy, testing the method on the hyper-parameters of a relatively simple Neural Network performing image classification.</p>
<p>The main conclusion of this research is that incredible computational resources are needed to characterize NN hyper-parameter optimization methods, see the Google Vizier “paper” for context.
Despite this, Bayesian Optimization is a promising technique for sequential, potentially stochastic, optimization problems where the cost of sampling is a limiting factor.
For example, see my paper with Alex Dunn, <em>Rocketsled</em>, on applications of Bayesian Optimization to Computational Materials Science.</p>
<p>In a broader context, stop searching for parameters by hand, and consider random search over grid search if the relative importance of each parameter is unknown (for example some may not be important at all).
See the <a href="http://www.jmlr.org/papers/v13/bergstra12a.html">paper</a> by Bergstra and Bengio for reference.
There is no reason to search parameters manually, there are many easy to use optimization codes to choose from, for example <a href="https://scikit-optimize.github.io/">skopt</a>.</p>
<h1 id="poster-pdf"><a href="/pdf/poster_CS682.pdf">Poster PDF</a></h1>
    </section>
  </article>
</main>
]]></description>
    <pubDate>Tue, 01 May 2018 00:00:00 UT</pubDate>
    <guid>https://spacedome.tv/posts/learning-hyper-parameters-with-bayesian-optimization.html</guid>
    <dc:creator>Julien</dc:creator>
</item>

    </channel>
</rss>
